Lecture 1: Models of Computation
===================

> **Lecture Overview:**

> - What is an algorithm? What is time?
> - Model of computation
> - Cache

----------

**What is an algorithm?**
> - Mathematical abstraction of computer program
> - Computational procedure to solve a problem
> - program = algorithm, programming language = pseudocode, computer = model of computation

“Is the algorithm good?” 
Well, that depends on how we define the “goodness” of an algorithm. In this class, as is typical, we will generally reframe this question as: “How does the running time of the algorithm grow with the size of the input?” Generally, the slower the running time grows as the input increases in size, the better the algorithm.

In particular, we will focus on **Asymptotic Analysis**. This type of analysis focuses on the running time of
your algorithm as your input size gets very large (i.e. n → +∞). This framework is motivated by the fact that if we need to solve a small problem, it doesn’t cost that much to solve it by brute-force. If we want to solve a large problem, we may need to be much more creative in order for the problem to run efficiently.

From this perspective, it should be very clear that **6n(log n + 1) = O(nlogn)** is much better than **n^2/2 = O(n^2)**. The constants that we’ve dropped will depend greatly on the language and machine on which you’re working

----------
**Model of computation specifies: **
• what operations an algorithm is allowed
• cost (time, space, . . .) of each operation
• cost of algorithm = sum of operation costs

**Random Access Machine (RAM) **

 The RAM model contains instructions commonly found in real computers: arithmetic (add, subtract, multiply, divide, remainder, floor, ceiling), data movement (load, store, copy), and control (conditional and unconditional branch, subroutine call and return). Each such instruction takes a constant amount of time.

For example, what if a RAM had an instruction that sorts? Then we could sort in just one instruction. Such a RAM would be unrealistic, since real computers do not have such instructions. 

In the RAM model, we do not attempt to model the memory hierarchy that is common in contemporary computers. That is, we do not model caches or virtual memory.

----------
 **Modern Memory Hierarchy**

In most data structure and algorithms classes, the model used for basic analysis is the traditional RAM model: we assume that we have a large, random-access array of memory, and count the number of simple reads/writes needed to perform the algorithm.

However, modern computers have memory hierarchy.

CPU ---> L1(10KB, 1ns) ---> L2(100KB, 10ns) ---> L3(MB, 10ns) ---> memory(GB, 100ns) ---> disk(TB, 10ms)
Each hierarchy on the right is bigger, but has longer latency due to the longer distance data has to travel.

For example, suppose you have a list of bytes stored in both an array and a linked list. Provided the linked list nodes are allocated at random positions, iterating over the array will be dramatically faster, exhibiting optimal locality of reference, while iterating over the linked list will trigger a cache miss for every element. We can formalize this: assume a cache line – the size of the block read into the cache at one time – has size B bytes. Then traversing the linked list requires n cache misses, while traversing the array requires precisely ceiling(n/B) cache misses, a speedup of B times.





